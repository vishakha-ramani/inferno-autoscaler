# workload-variant-autoscaler

![Version: 0.1.0](https://img.shields.io/badge/Version-0.1.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 1.0](https://img.shields.io/badge/AppVersion-1.0-informational?style=flat-square)

Helm chart for Workload-Variant-Autoscaler (WVA) - GPU-aware autoscaler for LLM inference workloads

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| hpa.enabled | bool | `true` |  |
| hpa.maxReplicas | int | `10` |  |
| hpa.targetAverageValue | string | `"1"` |  |
| llmd.modelID | string | `"unsloth/Meta-Llama-3.1-8B"` |  |
| llmd.modelName | string | `"ms-inference-scheduling-llm-d-modelservice"` |  |
| llmd.namespace | string | `"llm-d-inference-scheduler"` |  |
| va.accelerator | string | `"H100"` |  |
| va.enabled | bool | `true` |  |
| va.sloTpot | int | `10` |  |
| va.sloTtft | int | `1000` |  |
| vllmService.enabled | bool | `true` |  |
| vllmService.interval | string | `"15s"` |  |
| vllmService.nodePort | int | `30000` |  |
| vllmService.scheme | string | `"http"` |  |
| wva.enabled | bool | `true` |  |
| wva.experimentalProactiveModel | bool | `false` |  |
| wva.image.repository | string | `"ghcr.io/llm-d/workload-variant-autoscaler"` |  |
| wva.image.tag | string | `"latest"` |  |
| wva.imagePullPolicy | string | `"Always"` |  |
| wva.metrics.enabled | bool | `true` |  |
| wva.metrics.port | int | `8443` |  |
| wva.metrics.secure | bool | `true` |  |
| wva.prometheus.baseURL | string | `"https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"` |  |
| wva.prometheus.monitoringNamespace | string | `"openshift-user-workload-monitoring"` |  |
| wva.prometheus.tls.caCertPath | string | `"/etc/ssl/certs/prometheus-ca.crt"` |  |
| wva.prometheus.tls.insecureSkipVerify | bool | `true` |  |
| wva.reconcileInterval | string | `"60s"` |  |
| wva.scaleToZero | bool | `false` |  |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.14.2](https://github.com/norwoodj/helm-docs/releases/v1.14.2)

### INSTALL (on OpenShift)
1. Before running, be sure to delete all previous helm installations for workload-variant-scheduler and prometheus-adapter.
2. llm-d must be installed for WVA to do it's magic. If you plan on installing llm-d with these instructions, please be sure to remove any other helm installation of llm-d before proceeding.

#### NOTE: to view which helm charts you already have installed in your cluster, use:
```
helm ls -A
```

```
export OWNER="llm-d-incubation"
export WVA_PROJECT="workload-variant-autoscaler"
export WVA_RELEASE="v0.0.4"
export WVA_NS="workload-variant-autoscaler-system"
export MON_NS="openshift-user-workload-monitoring"

kubectl get secret thanos-querier-tls -n openshift-monitoring -o jsonpath='{.data.tls\.crt}' | base64 -d > /tmp/prometheus-ca.crt

git clone -b $WVA_RELEASE -- https://github.com/$OWNER/$WVA_PROJECT.git $WVA_PROJECT
cd $WVA_PROJECT
export WVA_PROJECT=$PWD
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm upgrade -i prometheus-adapter prometheus-community/prometheus-adapter \
  -n $MON_NS \
  -f config/samples/prometheus-adapter-values-ocp.yaml

kubectl apply -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: $WVA_NS
  labels:
    app.kubernetes.io/name: workload-variant-autoscaler
    control-plane: controller-manager
    openshift.io/user-monitoring: "true"
EOF

cd $WVA_PROJECT/charts
helm upgrade -i workload-variant-autoscaler ./workload-variant-autoscaler \
  -n $WVA_NS \
  --set-file wva.prometheus.caCert=/tmp/prometheus-ca.crt \
  --set va.accelerator=L40S \
  --set llmd.modelID=unsloth/Meta-Llama-3.1-8B \
  --set vllmService.enabled=true \
  --set vllmService.nodePort=30000
```

## Configuration Files

### Production vs Development Values

The Helm chart provides different configuration files for different environments:

#### Production Values (`values.yaml`)
- **TLS Verification**: Enabled (`insecureSkipVerify: false`)
- **Logging Level**: Production (`LOG_LEVEL: info`)
- **Security**: Strict security settings for production use
- **Capacity-based Scaling**: Conservative thresholds for production stability

#### Development Values (`values-dev.yaml`)
- **TLS Verification**: Relaxed (`insecureSkipVerify: true`) for easier development
- **Logging Level**: Debug (`LOG_LEVEL: debug`) for detailed development logging
- **Security**: Relaxed settings for development and testing
- **Capacity Scaling**: Aggressive thresholds for faster iteration

### Capacity Scaling Configuration

The chart includes capacity-based scaling thresholds that determine when replicas are saturated and when to scale up:

**Global Defaults** (applied to all models):
```yaml
wva:
  capacityScaling:
    default:
      kvCacheThreshold: 0.80      # Replica saturated if KV cache ≥ 80%
      queueLengthThreshold: 5     # Replica saturated if queue ≥ 5 requests
      kvSpareTrigger: 0.1         # Scale-up if spare KV capacity < 10%
      queueSpareTrigger: 3        # Scale-up if spare queue < 3
```

**Per-Model Overrides** (customize specific models):
```yaml
wva:
  capacityScaling:
    overrides:
      llm-d:
        modelID: "Qwen/Qwen3-0.6B"
        namespace: "llm-d-inference-scheduler"
        kvCacheThreshold: 0.70      # Lower threshold for production
        kvSpareTrigger: 0.35        # Avg spare KV <10% → scale-up
```

See `docs/capacity-scaling-config.md` for detailed configuration documentation.

### Usage Examples

#### Production Deployment
```bash
# Use production values (secure by default)
helm install workload-variant-autoscaler ./workload-variant-autoscaler \
  -n workload-variant-autoscaler-system \
  --values values.yaml
```

#### Development Deployment
```bash
# Use development values (relaxed security)
helm install workload-variant-autoscaler ./workload-variant-autoscaler \
  -n workload-variant-autoscaler-system \
  --values values-dev.yaml
```

#### Custom Configuration
```bash
# Override specific values
helm install workload-variant-autoscaler ./workload-variant-autoscaler \
  -n workload-variant-autoscaler-system \
  --values values.yaml \
  --set wva.prometheus.tls.insecureSkipVerify=true \
  --set wva.image.tag=v0.0.1-dev
```

### CLEANUP
```
export MON_NS="openshift-user-workload-monitoring"
export WVA_NS="workload-variant-autoscaler-system"

helm delete prometheus-adapter -n $MON_NS
helm delete workload-variant-autoscaler -n $WVA_NS
kubectl delete ns $WVA_NS
```

### VALIDATION / TROUBLESHOOTING
1. Check for 'error' in workload-variant-autoscaler-controller-manager-xxxxx in the workload-variant-autoscaler-system namespace
```
kubectl logs pod workload-variant-autoscaler-controller-manager-xxxxx -n workload-variant-autoscaler-system | grep error
```
2. Check for '404' in prometheus-adapter in the openshift-user-workload-monitoring namespace
```
kubectl logs pod prometheus-adapter-xxxxx -n openshift-user-workload-monitoring | grep 404
```
3. Check, after a few minutes following installation, for metric collection
```
kubectl get --raw "/apis/external.metrics.k8s.io/v1beta1/namespaces/$NAMESPACE/inferno_desired_replicas" | jq
```
