# Benchmark Jobs for Parameter Estimation
# This file provides templates for running synchronous and throughput benchmarks to estimate
# alpha/beta (decode) and gamma/delta (prefill) parameters.
#
# Customization required:
# 1. Replace <namespace> with your namespace (e.g., vllm-test)
# 2. Replace <vllm-service-name> with your vLLM service name (e.g., vllm)
# 3. Replace <model-id> with your model identifier (e.g., unsloth/Meta-Llama-3.1-8B)
# 4. Replace <max-batch-size> with your vLLM --max-num-seqs value (e.g., 64)
# 5. Adjust input_tokens and output_tokens in --data to match your workload
# 6. Adjust --max-seconds if you want different benchmark duration

---
apiVersion: batch/v1
kind: Job
metadata:
  name: <model-id>-sync-benchmark  # e.g., llama3-8b-sync-benchmark
  namespace: <namespace>
  labels:
    benchmark: parameter-estimation
    type: synchronous
spec:
  template:
    metadata:
      labels:
        benchmark: parameter-estimation
        type: synchronous
    spec:
      containers:
        - name: guidellm-benchmark-container
          image: ghcr.io/vllm-project/guidellm:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: HF_HOME
              value: "/tmp"
          command: ["guidellm"]
          args:
            - "benchmark"
            - "--target"
            - "http://<vllm-service-name>:8000"  # e.g., http://vllm:8000
            - "--rate-type"
            - "synchronous"
            - "--max-seconds"
            - "360"
            - "--model"
            - "<model-id>"  # e.g., unsloth/Meta-Llama-3.1-8B
            - "--data"
            - "prompt_tokens=128,output_tokens=128"
            - "--output-path"
            - "/tmp/benchmarks.json"
      restartPolicy: Never
  backoffLimit: 4
---
apiVersion: batch/v1
kind: Job
metadata:
  name: <model-id>-throughput-benchmark  # e.g., llama3-8b-throughput-benchmark
  namespace: <namespace>
  labels:
    benchmark: parameter-estimation
    type: throughput
spec:
  template:
    metadata:
      labels:
        benchmark: parameter-estimation
        type: throughput
    spec:
      containers:
        - name: guidellm-benchmark-container
          image: ghcr.io/vllm-project/guidellm:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: HF_HOME
              value: "/tmp"
            - name: GUIDELLM__MAX_CONCURRENCY
              value: "<max-batch-size>"  # Must match vLLM --max-num-seqs (e.g., "64")
          command: ["guidellm"]
          args:
            - "benchmark"
            - "--target"
            - "http://<vllm-service-name>:8000"  # e.g., http://vllm:8000
            - "--rate-type"
            - "throughput"
            - "--max-seconds"
            - "360"
            - "--model"
            - "<model-id>"  # e.g., unsloth/Meta-Llama-3.1-8B
            - "--data"
            - "prompt_tokens=128,output_tokens=128"
            - "--output-path"
            - "/tmp/benchmarks.json"
      restartPolicy: Never
  backoffLimit: 4

