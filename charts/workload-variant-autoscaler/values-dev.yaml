# Development values for workload-variant-autoscaler
# This file contains development-specific configurations with relaxed security settings

wva:
  enabled: true
  baseName: inference-scheduling
  replicaCount: 1
  image:
    repository: ghcr.io/llm-d/workload-variant-autoscaler
    tag: latest
  imagePullPolicy: Always

  reconcileInterval: 30s

  modelName: ms-inference-scheduling-llm-d-modelservice

  metrics:
    enabled: true
    port: 8443
    secure: true
  
  # Development logging configuration
  logging:
    level: debug  # Development: debug, Production: info
  prometheus:
    monitoringNamespace: openshift-user-workload-monitoring
    baseURL: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
    # Development security configuration (relaxed for easier development)
    tls:
      insecureSkipVerify: true   # Development: true, Production: false
      caCertPath: "/etc/ssl/certs/prometheus-ca.crt"
    # caCert: |  # Uncomment and provide your CA certificate
    #   -----BEGIN CERTIFICATE-----
    #   YOUR_CA_CERTIFICATE_HERE
    #   -----END CERTIFICATE-----

  experimentalHybridOptimization: "off"  # Enable experimental hybrid optimization (default: "off")
  scaleToZero: false  # Enable scaling variants to zero replicas (default: false)

  # Capacity-based scaling configuration
  # These thresholds determine when replicas are saturated and when to scale up
  capacityScaling:
    # Global defaults applied to all variants unless overridden
    default:
      kvCacheThreshold: 0.80      # Replica saturated if KV cache utilization >= threshold (0.0-1.0)
      queueLengthThreshold: 5     # Replica saturated if queue length >= threshold
      kvSpareTrigger: 0.1         # Scale-up if avg spare KV capacity < trigger (0.0-1.0)
      queueSpareTrigger: 3        # Scale-up if avg spare queue capacity < trigger
    
    # Per-model/namespace overrides (optional)
    # Example:
    # overrides:
    #   llm-d:
    #     modelID: "Qwen/Qwen3-0.6B"
    #     namespace: "llm-d-inference-scheduler"
    #     kvCacheThreshold: 0.70
    #     kvSpareTrigger: 0.35
    overrides: {}

llmd:
  namespace: llm-d-inference-scheduling
  modelName: ms-inference-scheduling-llm-d-modelservice
  modelID: "unsloth/Meta-Llama-3.1-8B"
va:
  enabled: true
  accelerator: H100
  sloTpot: 10
  sloTtft: 1000
hpa:
  enabled: true
  maxReplicas: 10
  targetAverageValue: "1"
vllmService:
  enabled: true
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
