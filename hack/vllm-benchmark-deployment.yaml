# vLLM Deployment for Benchmarking
# This file provides a template for deploying vLLM for parameter estimation benchmarking.
# 
# Customization required:
# 1. Replace <namespace> with your target namespace (e.g., vllm-test)
# 2. Replace <vllm-service-name> with your desired service name (e.g., vllm)
# 3. Replace <model-id> with your model identifier (e.g., unsloth/Meta-Llama-3.1-8B)
# 4. Replace <secret-name> with your HF token secret name (e.g., hf-token-secret)
# 5. Replace <pvc-name> with your PVC name for model cache (e.g., vllm-models-cache)
# 6. Adjust --max-num-seqs value to match your desired maximum batch size (default: 64)
# 7. Adjust GPU resource requests/limits based on your GPU type (A100, H100, L40S, etc.)
# 8. Adjust memory limits based on model size and GPU memory

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: <vllm-service-name>  # e.g., vllm
  namespace: <namespace>      # e.g., vllm-test
  labels:
    app: <vllm-service-name>
spec:
  replicas: 1
  selector:
    matchLabels:
      app: <vllm-service-name>
  template:
    metadata:
      labels:
        app: <vllm-service-name>
    spec:
      restartPolicy: Always
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 120
      containers:
        - name: vllm-server
          image: vllm/vllm-openai:latest
          imagePullPolicy: Always
          command: ["/bin/bash", "-c"]
          args:
            - |
              vllm serve <model-id> \
              --trust-remote-code \
              --download-dir /models-cache \
              --dtype float16 \
              --max-num-seqs 64 \
              --port 8000
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: <secret-name>  # e.g., hf-token-secret
                  key: token
            - name: HF_HOME
              value: /models-cache
            - name: HOME
              value: /models-cache
            - name: VLLM_PORT
              value: "8000"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          resources:
            limits:
              cpu: "8"
              memory: 40Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "6"
              memory: 40Gi
              nvidia.com/gpu: "1"
          readinessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 8
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 24
          volumeMounts:
            - name: models-cache
              mountPath: /models-cache
            - name: shm
              mountPath: /dev/shm
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
      volumes:
        - name: models-cache
          persistentVolumeClaim:
            claimName: <pvc-name>  # e.g., vllm-models-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      dnsPolicy: ClusterFirst
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
---
apiVersion: v1
kind: Service
metadata:
  name: <vllm-service-name>  # Must match deployment name
  namespace: <namespace>
  labels:
    app: <vllm-service-name>
spec:
  ports:
    - name: http
      protocol: TCP
      port: 8000
      targetPort: http
  selector:
    app: <vllm-service-name>
  type: ClusterIP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: <vllm-service-name>-monitor
  namespace: <namespace>
  labels:
    app: <vllm-service-name>
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: <vllm-service-name>
  endpoints:
    - port: http
      interval: 15s
      path: /metrics
  namespaceSelector:
    any: true

